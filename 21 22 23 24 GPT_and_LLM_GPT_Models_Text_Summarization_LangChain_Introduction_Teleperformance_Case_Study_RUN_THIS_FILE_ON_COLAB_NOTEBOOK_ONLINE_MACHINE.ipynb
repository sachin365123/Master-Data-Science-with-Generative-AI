{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "55db2089-9ccf-4fa2-996e-3fc8b3e12733",
      "metadata": {
        "id": "55db2089-9ccf-4fa2-996e-3fc8b3e12733"
      },
      "source": [
        "**-----------------------------------------------------------------------------------------------------------------**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd920170-af15-45bf-a429-630356ec976f",
      "metadata": {
        "id": "bd920170-af15-45bf-a429-630356ec976f"
      },
      "source": [
        "*In this lecture we are going to explore:*\n",
        "\n",
        "1. What are GPT models?\n",
        "2. How GPT models Works?\n",
        "3. Python implementation of LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82bc589-0789-4097-8cc1-c2a449563cb7",
      "metadata": {
        "id": "f82bc589-0789-4097-8cc1-c2a449563cb7"
      },
      "source": [
        "**-----------------------------------------------------------------------------------------------------------------**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4713ec59-f2fb-43b8-8a61-7ba20317c602",
      "metadata": {
        "id": "4713ec59-f2fb-43b8-8a61-7ba20317c602"
      },
      "source": [
        "![1 GPT 1.png](attachment:02b91b3c-4bc1-41f1-ad2e-5acaf43bcd6e.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30bb8010-5e7b-4532-ac72-53321f28adc1",
      "metadata": {
        "id": "30bb8010-5e7b-4532-ac72-53321f28adc1"
      },
      "source": [
        "* A brief illustration for the technical evolution of GPT-series models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699d5c32-73c7-4e7c-8f56-083a1189adeb",
      "metadata": {
        "id": "699d5c32-73c7-4e7c-8f56-083a1189adeb"
      },
      "source": [
        "* Generative pretrained transformers (GPTs) are a family of large language models (LLMs) based on a transformer deep learning architecture.\n",
        "* Developed by OpenAI, these foundation models power ChatGPT and other generative AI applications capable of simulating human-created output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63db5c28-ab40-49b3-a4da-ddffc36fe2d9",
      "metadata": {
        "id": "63db5c28-ab40-49b3-a4da-ddffc36fe2d9"
      },
      "source": [
        "* **Why is GPT important?**\n",
        "* GPT models have accelerated generative AI development thanks to their transformer architecture, a type of neural network introduced in 2017 in the Google Brain paper Attention Is All You Need2.\n",
        "* Transformer models including GPT and BERT have powered many notable developments in generative AI since then, with OpenAI’s ChatGPT chatbot taking center stage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352421fc-ee51-4298-b2df-e3e248a52b4c",
      "metadata": {
        "id": "352421fc-ee51-4298-b2df-e3e248a52b4c"
      },
      "source": [
        "![1 GPT 2.png](attachment:c2bdffd5-04b7-4c17-a573-ae8c83cd5fdf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e82f88-e1b9-46b1-99fb-f0009f4ccff5",
      "metadata": {
        "id": "e4e82f88-e1b9-46b1-99fb-f0009f4ccff5"
      },
      "source": [
        "* An illustration of a typical data preprocessing pipeline for pre-training large language models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee752703-4e81-4faf-a155-d4ce9fad91c9",
      "metadata": {
        "id": "ee752703-4e81-4faf-a155-d4ce9fad91c9"
      },
      "source": [
        "* Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data.\n",
        "* The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities.\n",
        "* The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1cc7dab-eec5-429b-b40b-a6c81a76272c",
      "metadata": {
        "id": "a1cc7dab-eec5-429b-b40b-a6c81a76272c"
      },
      "source": [
        "* **Three common learning models exist:**\n",
        "  \n",
        "1. Zero-shot learning; Base LLMs can respond to a broad range of requests without explicit training, often through prompts, although answer accuracy varies.\n",
        "2. Few-shot learning: By providing a few relevant training examples, base model performance significantly improves in that specific area.\n",
        "3. Fine-tuning: This is an extension of few-shot learning in that data scientists train a base model to adjust its parameters with additional data relevant to the specific application."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a88355c-14a8-42b7-8c15-96b16556f0b1",
      "metadata": {
        "id": "7a88355c-14a8-42b7-8c15-96b16556f0b1"
      },
      "source": [
        "![1 GPT 3.png](attachment:5dbe3c27-2772-422e-8849-41164c6909a6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "567154ba-1905-49a6-a113-93efb4d32160",
      "metadata": {
        "id": "567154ba-1905-49a6-a113-93efb4d32160"
      },
      "source": [
        "# How do LLMs work?\n",
        "* A simplified version of the LLM training process\n",
        "\n",
        "![1 GPT 4.png](attachment:33e08c6c-3899-4b12-b0b2-39614dd7a4bc.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc1f964-5f60-4d2c-a401-0996e27f81ec",
      "metadata": {
        "id": "6dc1f964-5f60-4d2c-a401-0996e27f81ec"
      },
      "source": [
        "References: https://www.databricks.com/glossary/large-language-models-llm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f2b57d",
      "metadata": {
        "id": "23f2b57d"
      },
      "source": [
        "## Setting up the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6342a76",
      "metadata": {
        "id": "d6342a76"
      },
      "outputs": [],
      "source": [
        "import openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZiSOyXY3ZBNS",
      "metadata": {
        "id": "ZiSOyXY3ZBNS"
      },
      "outputs": [],
      "source": [
        "api_key=\"paste your secret key here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12df389",
      "metadata": {
        "id": "d12df389"
      },
      "outputs": [],
      "source": [
        "openai.api_key = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q9LNvtS0a2Fr",
      "metadata": {
        "id": "q9LNvtS0a2Fr"
      },
      "outputs": [],
      "source": [
        "# Make payment or check from here\n",
        "# https://platform.openai.com/usage\n",
        "# https://platform.openai.com/settings/organization/billing/overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "691da4eb",
      "metadata": {
        "id": "691da4eb"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ug-7wmUPcFVU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "Ug-7wmUPcFVU",
        "outputId": "2e80e952-b608-4a49-b9b7-ea58e1dbe195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "Successfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "e0bdc93b5bff42bea2da982e67e5817b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zw8ES2Vec1p7",
      "metadata": {
        "id": "Zw8ES2Vec1p7"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci-002\", #Specify the model we want to use\n",
        "        prompt=prompt,\n",
        "        max_tokens=10,        #Maximum Number of tokens\n",
        "        temperature=0.7)\n",
        "    return response.choices[0].text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e99cb2c",
      "metadata": {
        "id": "1e99cb2c"
      },
      "outputs": [],
      "source": [
        "prompt = \"Once upon a time\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lYjUIUnrc8V7",
      "metadata": {
        "id": "lYjUIUnrc8V7"
      },
      "outputs": [],
      "source": [
        "# prompt =\"Write a short poem about AI\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb813243",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb813243",
        "outputId": "19f359aa-77ce-4472-8d00-00513447dc21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time , there was a princess called Cinderella. Her\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_text(prompt)\n",
        "print(prompt, generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8ff65b",
      "metadata": {
        "id": "8e8ff65b"
      },
      "source": [
        "## Customizing the Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6815d2a1",
      "metadata": {
        "id": "6815d2a1"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_tokens, temperature):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci-002\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature)\n",
        "    return response.choices[0].text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372acc3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "372acc3b",
        "outputId": "2c39f351-ff02-4e7a-b5f2-f877abd5f002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time , there was a little girl who was born with a very special gift. She could see things that others could not. She could see the future, and she could see the past. She could see the present, and she could see the future.\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_text(prompt, 50, 0)\n",
        "print(prompt, generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YEEDDpU2e3WL",
      "metadata": {
        "id": "YEEDDpU2e3WL"
      },
      "outputs": [],
      "source": [
        "# It makes sense where the temperature is one.\n",
        "\n",
        "# It will be more random.\n",
        "\n",
        "# But because we've limited the number of tokens, it's quite hard to see that.\n",
        "\n",
        "# So let's up our number of max tokens to 50.\n",
        "\n",
        "# So when we allow the max tokens as 50 and the temperature of zero, we get the following response.\n",
        "\n",
        "# Once upon a time, there was a little girl who was born with a very special gift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9295bd47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9295bd47",
        "outputId": "13981423-afa9-48b5-d26b-d1deff55563b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in the magic land of Yokohama, the kingdom of misfits, the esoteric meets the offbeat.\n",
            "\n",
            "How do you start a small chapel for a funky little place like Yokohama? If you are homage Motoyuki Haneda,\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_text(prompt, 50, 1)\n",
        "print(prompt, generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4554f553",
      "metadata": {
        "id": "4554f553"
      },
      "source": [
        "## Summarising Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Toj9HHpUfS5y",
      "metadata": {
        "id": "Toj9HHpUfS5y"
      },
      "outputs": [],
      "source": [
        "# we're going to learn how to create a function that can be given a large chunk of text and summarize\n",
        "\n",
        "# it by picking out the key words.\n",
        "\n",
        "# Because the model is able to understand language, it is then able to pick out the most important words\n",
        "\n",
        "# and phrases for us.\n",
        "\n",
        "# We create our function and provide messages to the model with instructions and examples of how we want\n",
        "\n",
        "# the model to respond.\n",
        "\n",
        "# There are three roles that can be used for messages.\n",
        "\n",
        "# The first messages are system messages.\n",
        "\n",
        "# These messages provide instructions to the model on what you want it to do and how it should respond\n",
        "\n",
        "# and behave right.\n",
        "\n",
        "# The second type of message is a user message.\n",
        "\n",
        "# This is an example of a user input.\n",
        "\n",
        "# So in this case the user input would be the block of text we want to summarize.\n",
        "\n",
        "# The third type of message is an assistant message.\n",
        "\n",
        "# This is an example of the correct output to the user input.\n",
        "\n",
        "# So here we have key words that have been pulled out from the block of text provided by the user.\n",
        "\n",
        "# These assistant messages serve as an example of how the model should respond."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0fc98cd",
      "metadata": {
        "id": "c0fc98cd"
      },
      "outputs": [],
      "source": [
        "def text_summarizer(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You will be provided with a block of text, and your task is to extract a list of keywords from it.\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"A flying saucer seen by a guest house, a 7ft alien-like figure coming out of a hedge and a \\\"cigar-shaped\\\" UFO near a school yard.\\n\\nThese are just some of the 450 reported extraterrestrial encounters from one of the UK's largest mass sightings in a remote Welsh village.\\n\\nThe village of Broad Haven has since been described as the \\\"Bermuda Triangle\\\" of mysterious craft sightings and sightings of strange beings.\\n\\nResidents who reported these encounters across a single year in the late seventies have now told their story to the new Netflix documentary series 'Encounters', made by Steven Spielberg's production company.\\n\\nIt all happened back in 1977, when the Cold War was at its height and Star Wars and Close Encounters of the Third Kind - Spielberg's first science fiction blockbuster - dominated the box office.\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"assistant\",\n",
        "          \"content\": \"flying saucer, guest house, 7ft alien-like figure, hedge, cigar-shaped UFO, school yard, extraterrestrial encounters, UK, mass sightings, remote Welsh village, Broad Haven, Bermuda Triangle, mysterious craft sightings, strange beings, residents, single year, late seventies, Netflix documentary series, Steven Spielberg, production company, 1977, Cold War, Star Wars, Close Encounters of the Third Kind, science fiction blockbuster, box office.\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Each April, in the village of Maeliya in northwest Sri Lanka, Pinchal Weldurelage Siriwardene gathers his community under the shade of a large banyan tree. The tree overlooks a human-made body of water called a wewa – meaning reservoir or \\\"tank\\\" in Sinhala. The wewa stretches out besides the village's rice paddies for 175-acres (708,200 sq m) and is filled with the rainwater of preceding months.    \\n\\nSiriwardene, the 76-year-old secretary of the village's agrarian committee, has a tightly-guarded ritual to perform. By boiling coconut milk on an open hearth beside the tank, he will seek blessings for a prosperous harvest from the deities residing in the tree. \\\"It's only after that we open the sluice gate to water the rice fields,\\\" he told me when I visited on a scorching mid-April afternoon.\\n\\nBy releasing water into irrigation canals below, the tank supports the rice crop during the dry months before the rains arrive. For nearly two millennia, lake-like water bodies such as this have helped generations of farmers cultivate their fields. An old Sinhala phrase, \\\"wewai dagabai gamai pansalai\\\", even reflects the technology's centrality to village life; meaning \\\"tank, pagoda, village and temple\\\".\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"assistant\",\n",
        "          \"content\": \"April, Maeliya, northwest Sri Lanka, Pinchal Weldurelage Siriwardene, banyan tree, wewa, reservoir, tank, Sinhala, rice paddies, 175-acres, 708,200 sq m, rainwater, agrarian committee, coconut milk, open hearth, blessings, prosperous harvest, deities, sluice gate, rice fields, irrigation canals, dry months, rains, lake-like water bodies, farmers, cultivate, Sinhala phrase, technology, village life, pagoda, temple.\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": prompt\n",
        "        }\n",
        "      ],\n",
        "      temperature=0.5,\n",
        "      max_tokens=256\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272cc352",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "272cc352",
        "outputId": "a4f63b2c-990f-482a-f719-a6336f718419",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master Reef Guide Kirsty Whitman didn't need to tell me twice. Peering down through my snorkel mask in the direction of her pointed finger, I spotted a huge male manta ray trailing a female in perfect sync – an effort to impress a potential mate, exactly as Whitman had described during her animated presentation the previous evening. Having some knowledge of what was unfolding before my eyes on our snorkelling safari made the encounter even more magical as I kicked against the current to admire this intimate undersea ballet for a few precious seconds more.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Master Reef Guide Kirsty Whitman didn't need to tell me twice. Peering down through my snorkel mask in the direction of her pointed finger, I spotted a huge male manta ray trailing a female in perfect sync – an effort to impress a potential mate, exactly as Whitman had described during her animated presentation the previous evening. Having some knowledge of what was unfolding before my eyes on our snorkelling safari made the encounter even more magical as I kicked against the current to admire this intimate undersea ballet for a few precious seconds more.\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed81104f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ed81104f",
        "outputId": "3ce5aea5-2cf1-4518-ae52-596ec26f1b46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Master Reef Guide, Kirsty Whitman, snorkel mask, male manta ray, female, sync, potential mate, presentation, snorkelling safari, encounter, magical, current, undersea ballet.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "text_summarizer(prompt)\n",
        "# When we print this generated text out, we can see that it's the key words from our piece of text above.\n",
        "\n",
        "# In this lesson, we've seen how we're able to provide messages to our model in order to get it to work\n",
        "\n",
        "# with different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e189025b",
      "metadata": {
        "id": "e189025b"
      },
      "source": [
        "## Poetic Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8os0Hk5Ig0Ik",
      "metadata": {
        "id": "8os0Hk5Ig0Ik"
      },
      "outputs": [],
      "source": [
        "# In the poetic_chatbot function, the messages parameter in openai.ChatCompletion.create() is a list that defines the conversation history.\n",
        "\n",
        "# Each message in the list has a role (\"system\", \"user\", or \"assistant\") and a content field that contains the actual text.\n",
        "\n",
        "# Message Breakdown:\n",
        "# System Message (\"role\": \"system\")\n",
        "\n",
        "# \"content\": \"You are a poetic chatbot.\"\n",
        "# This sets the AI’s behavior, instructing it to respond poetically.\n",
        "# User Messages (\"role\": \"user\")\n",
        "\n",
        "# These represent the questions asked by the user.\n",
        "# Example: \"content\": \"When was Google founded?\"\n",
        "# Assistant Messages (\"role\": \"assistant\")\n",
        "\n",
        "# These are the AI’s poetic responses.\n",
        "# Example: \"content\": \"In the late '90s, a spark did ignite...\"\n",
        "# Dynamic User Prompt (\"role\": \"user\")\n",
        "\n",
        "# The final message dynamically inserts the prompt parameter, meaning whatever input the user provides will be added to the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c00bf8",
      "metadata": {
        "id": "e8c00bf8"
      },
      "outputs": [],
      "source": [
        "def poetic_chatbot(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model = \"gpt-3.5-turbo\",\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a poetic chatbot.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"When was Google founded?\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"In the late '90s, a spark did ignite, Google emerged, a radiant light. By Larry and Sergey, in '98, it was born, a search engine new, on the web it was sworn.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Which country has the youngest president?\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Ah, the pursuit of youth in politics, a theme we explore. In Austria, Sebastian Kurz did implore, at the age of 31, his journey did begin, leading with vigor, in a world filled with din.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature = 1,\n",
        "        max_tokens=256\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6041f72f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6041f72f",
        "outputId": "c422863c-9afe-42d1-d6db-ad6bfd4f5013"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Eons ago, in ancient times unspoken, the art of cheese-making was awoken. Perhaps by accident or divine inspiration, milk transformed into a wondrous creation. Millennia have passed since that fateful day, yet cheese remains a delicacy in every way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "prompt = \"When was cheese first made?\"\n",
        "poetic_chatbot(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5afa683",
      "metadata": {
        "id": "c5afa683"
      },
      "source": [
        "## Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V62cp3qzhjUD",
      "metadata": {
        "id": "V62cp3qzhjUD"
      },
      "outputs": [],
      "source": [
        "# the data it's been trained on.\n",
        "\n",
        "# Let's see what happens when we ask a more specific question to this model.\n",
        "\n",
        "# So we can create a new prompt that asks, what is the next course to be uploaded on the 365 data Science\n",
        "\n",
        "# platform?\n",
        "\n",
        "# As.\n",
        "\n",
        "# You can see that the model has given a response, but there's no actual answer in this response.\n",
        "\n",
        "# The model isn't able to tell us which course will be uploaded next onto the 365 data science platform,\n",
        "\n",
        "# because it just doesn't have this data available.\n",
        "\n",
        "# GPT models have been trained on a lot of data, but this data only goes as far as 2021, so they might\n",
        "\n",
        "# not have the most up to date information.\n",
        "\n",
        "# But there is a solution.\n",
        "\n",
        "# Using Lang Chain, we are able to import our own data and have this read by our language models.\n",
        "\n",
        "# Our language models can then reference this when creating a response using the Lang chain framework.\n",
        "\n",
        "# So let's have a look at what Lang chain is and what it can be used for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac09b08f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ac09b08f",
        "outputId": "4842b946-5d5a-4a9d-fc4c-4c8cf55d7a10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, the mysteries of courses new, an adventure awaits, a journey to pursue. In the realm of 365DataScience, knowledge unfolds, a treasure trove of insights, a tale untold. Keep your eyes peeled, the next course soon to grace, expanding minds, a learning space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "prompt = \"What is the next course to be uploaded to 365DataScience?\"\n",
        "poetic_chatbot(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KLKn8Iq6hw9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLKn8Iq6hw9b",
        "outputId": "0cae0fe5-3107-483d-eef8-2c1586f34956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.40)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.18 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90710771",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90710771",
        "outputId": "98c8aca7-b235-404e-89bc-1b529b64166f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722f492b",
      "metadata": {
        "id": "722f492b"
      },
      "outputs": [],
      "source": [
        "url = \"https://365datascience.com/upcoming-courses\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73fc8b53",
      "metadata": {
        "id": "73fc8b53"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96219fdb",
      "metadata": {
        "id": "96219fdb"
      },
      "outputs": [],
      "source": [
        "raw_documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y3LuLjAAiuos",
      "metadata": {
        "id": "y3LuLjAAiuos"
      },
      "outputs": [],
      "source": [
        "# The next step is to take our data and chunk it up into small pieces.\n",
        "\n",
        "# This is necessary in order to make sure we only pass the smallest, most relevant pieces of text to\n",
        "\n",
        "# the language model.\n",
        "\n",
        "# So we'll create our text splitter.\n",
        "\n",
        "# And then use the recursive character text splitter.\n",
        "\n",
        "# We can then create our documents.\n",
        "\n",
        "# So we'll use text splitter dot split documents over our raw documents.\n",
        "\n",
        "# This will take the text from our URL and break it up into small pieces for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368905d4",
      "metadata": {
        "id": "368905d4"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(raw_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb11a9aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb11a9aa",
        "outputId": "5631113a-180d-47fb-833e-7ee32534dfc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-cbe13bf5d25b>:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(openai_api_key = api_key)\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key = api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QckuOexyiQfj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QckuOexyiQfj",
        "outputId": "f0117642-e3d6-4b09-bb0b-3b1cb5dca3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_vq6PrxtiXNT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vq6PrxtiXNT",
        "outputId": "615d46b7-5ab4-4c43-c58c-56e99b99de94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "# FAISS (Facebook AI Similarity Search)\n",
        "\n",
        "# or, if you're using GPUs:\n",
        "\n",
        "# !pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822e2065",
      "metadata": {
        "id": "822e2065"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eae330f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eae330f",
        "outputId": "96261fd8-5ec3-4fcc-d80f-19009533eb06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-1b1d393162d5>:13: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "# We can now create a memory object which is necessary to track the inputs and outputs, and for the model\n",
        "\n",
        "# to hold a conversation.\n",
        "\n",
        "# So to create our memory we use conversation buffer memory.\n",
        "\n",
        "# We'll give the memory keys as the chat history.\n",
        "\n",
        "# And specify return messages equals true.\n",
        "\n",
        "# We can now initialize the conversational retrieval chain.\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab290f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ab290f6",
        "outputId": "cb6a0cdf-cb11-4f39-defe-030d45ee75e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-72806e98645f>:19: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(openai_api_key=api_key,\n"
          ]
        }
      ],
      "source": [
        "# We'll use conversational retrieval chain from LM.\n",
        "\n",
        "# We then provide our OpenAI API key.\n",
        "\n",
        "# And specify the temperature of the response.\n",
        "\n",
        "# If you remember the temperature just controls how random the response is.\n",
        "\n",
        "# We then pass our vector store as retriever.\n",
        "\n",
        "# And pass our memory.\n",
        "\n",
        "# And that's it.\n",
        "\n",
        "# We're now ready to ask our model questions based on this text we provided from the 365 Data Science\n",
        "\n",
        "# website.\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(openai_api_key=api_key,\n",
        "                                                  model=\"gpt-3.5-turbo\",\n",
        "                                                  temperature=0),\n",
        "                                           vectorstore.as_retriever(),\n",
        "                                           memory=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1520ae1",
      "metadata": {
        "id": "b1520ae1"
      },
      "outputs": [],
      "source": [
        "query = \"What is the next course to be uploaded on the 365DataScience platform?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29986146",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29986146",
        "outputId": "8a33d0cc-00cb-426a-b0a9-d60b92e2b742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-283ce1c4cb52>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa({\"question\": query})\n"
          ]
        }
      ],
      "source": [
        "result = qa({\"question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d181fc45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "d181fc45",
        "outputId": "dd07bd2c-057b-458c-8027-e2a3e2f3343c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The next course to be uploaded on the 365DataScience platform is \"AI Ethics\" with instructor Ned Krastev. The course is set to launch in March 2025.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d911ace",
      "metadata": {
        "id": "7d911ace"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "longchain_env",
      "language": "python",
      "name": "longchain_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}