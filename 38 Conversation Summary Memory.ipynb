{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c93bf58",
   "metadata": {},
   "source": [
    "# Conversation Summary Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4b449-a6ba-4a17-a8c0-0b26c7820d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the line of code below to check the version of langchain in the current environment.\n",
    "# Substitute \"langchain\" with any other package name to check their version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd4575-624b-45d2-96d4-733486b1d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35146cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baedcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "# ConversationSummaryMemory: New Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0bbe5-d256-430c-a09f-e8ae5b1e6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why ConversationSummaryMemory\n",
    "# So far in this section, we've covered the conversation buffer and conversation buffer window memory\n",
    "# classes.\n",
    "# Recall that as opposed to the buffer memory, which stacks all chat messages from a conversation, the\n",
    "# buffer window memory imposes a cap on the number of human I interactions that can be saved.\n",
    "# This ensures that long conversations won't slow down the response time and wouldn't significantly increase\n",
    "# the number of prompt tokens.\n",
    "# Still, the buffer window memory class has the disadvantage that the chat bot will eventually lose the\n",
    "# memory of earlier topics.\n",
    "# Furthermore, the buffer and the buffer window memory classes store the original messages word by word,\n",
    "# which isn't the most efficient approach for implementing chat bot memory.\n",
    "# For example, if you try to remember a conversation with a friend a week ago.\n",
    "# You won't likely recall the dialogue verbatim.\n",
    "# Instead, you'd summarize only the most essential information.\n",
    "# That's precisely the idea implemented in the conversation summary memory class.\n",
    "# Let's give it a go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f5a13d-17eb-4db7-8ba1-f10b304d31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815211b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\longchain_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3519: UserWarning: Parameters {'seed'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name = 'gpt-4', \n",
    "                  model_kwargs = {'seed':365},\n",
    "                  temperature = 0,\n",
    "                  max_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433408d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{message_log}\n",
    "\n",
    "Human:\n",
    "{question}\n",
    "\n",
    "AI:\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template = TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd66f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16144\\1830851948.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = ConversationSummaryMemory(llm = ChatOpenAI(),\n"
     ]
    }
   ],
   "source": [
    "chat_memory = ConversationSummaryMemory(llm = ChatOpenAI(), \n",
    "                                        memory_key = 'message_log', \n",
    "                                        return_messages = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221632c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd613d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16144\\432851078.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = chat,\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = chat, \n",
    "                 prompt = prompt_template, \n",
    "                 memory = chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82d9b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI. \n",
      "The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "Human:\n",
      "Can you give me an interesting fact I probably didn't know about?\n",
      "\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Sure, did you know that octopuses have three hearts? Two pump blood to the gills, while the third pumps it to the rest of the body.\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you give me an interesting fact I probably didn't know about?\",\n",
       " 'message_log': '',\n",
       " 'text': 'Sure, did you know that octopuses have three hearts? Two pump blood to the gills, while the third pumps it to the rest of the body.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"Can you give me an interesting fact I probably didn't know about?\"})\n",
    "# chain.invoke({'question':\"Can you elaborate a bit more on this fact?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "958f7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI. \n",
      "The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks the AI to share an interesting fact. The AI reveals that octopuses have three hearts - two for pumping blood to the gills and one for the rest of the body.\n",
      "\n",
      "Human:\n",
      "Can you elaborate a bit more on this fact?\n",
      "\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "The human asks the AI to share an interesting fact. The AI reveals that octopuses have three hearts - two for pumping blood to the gills and one for the rest of the body.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you elaborate a bit more on this fact?\n",
      "AI: Absolutely! Octopuses are fascinating creatures with a unique biological structure. As I mentioned, they have three hearts. Two of these hearts, known as branchial hearts, are located near each of the octopus's two gills. These hearts pump blood to the gills where it gets oxygenated. \n",
      "\n",
      "The third heart, known as the systemic heart, pumps the oxygenated blood from the gills to the rest of the body. However, when an octopus swims, the systemic heart actually\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Can you elaborate a bit more on this fact?',\n",
       " 'message_log': 'The human asks the AI to share an interesting fact. The AI reveals that octopuses have three hearts - two for pumping blood to the gills and one for the rest of the body.',\n",
       " 'text': \"Absolutely! Octopuses are fascinating creatures with a unique biological structure. As I mentioned, they have three hearts. Two of these hearts, known as branchial hearts, are located near each of the octopus's two gills. These hearts pump blood to the gills where it gets oxygenated. \\n\\nThe third heart, known as the systemic heart, pumps the oxygenated blood from the gills to the rest of the body. However, when an octopus swims, the systemic heart actually\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.invoke({'question':\"Can you give me an interesting fact I probably didn't know about?\"})\n",
    "chain.invoke({'question':\"Can you elaborate a bit more on this fact?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f1555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': \"The human asks the AI to share an interesting fact. The AI reveals that octopuses have three hearts - two for pumping blood to the gills and one for the rest of the body. The AI explains in detail about the unique structure of an octopus's hearts and how they function.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729999c-a771-47d9-8a42-896c52616203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.invoke({'question':\"Can you give me an interesting fact I probably didn't know about?\"})\n",
    "# chain.invoke({'question':\"Can you elaborate a bit more on this fact?\"})\n",
    "chain.invoke({'question':\"Can you find scitific reason of this fact?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longchain_env",
   "language": "python",
   "name": "longchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
