{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294d1be6",
   "metadata": {},
   "source": [
    "# RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3cebf7-7c60-445b-8646-3c9d1569748f",
   "metadata": {},
   "source": [
    "### Run the line of code below to check the version of langchain in the current environment.\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7597ba62-c4fb-46a2-96de-f73422c91800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.20\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\hp\\anaconda3\\envs\\longchain_envllm\\lib\\site-packages\n",
      "Requires: async-timeout, langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: langchain-community\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645e9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0553d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sum = lambda x: sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b40542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sum([1, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c9bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_square = lambda x: x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27766a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_square(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7478f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_sum = RunnableLambda(lambda x: sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a4f909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_sum.invoke([1, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2902b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_square = RunnableLambda(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d37bdecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_square.invoke(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9990edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = runnable_sum | runnable_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef69d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke([1, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aa034fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+  \n",
      "| LambdaInput |  \n",
      "+-------------+  \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "   +--------+    \n",
      "   | Lambda |    \n",
      "   +--------+    \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "   +--------+    \n",
      "   | Lambda |    \n",
      "   +--------+    \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "+--------------+ \n",
      "| LambdaOutput | \n",
      "+--------------+ \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e4ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f27444d6-96f1-448b-9a76-d6c022c4435a",
   "metadata": {},
   "source": [
    "# The @chain Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e03d3e0-fd08-45b2-a641-8bf48fdd139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55233340-e2c3-4c85-9e59-a9857bd263a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sum(x):\n",
    "    return sum(x)\n",
    "\n",
    "def find_square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23b4b0be-f81f-4b1f-8743-ca7e0986918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = RunnableLambda(find_sum) | RunnableLambda(find_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f19d71df-efd4-4f81-b128-a9dddd1b9c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1.invoke([1, 2, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272dddb7-c517-4e68-a43c-eed2a2370427",
   "metadata": {},
   "source": [
    "#### Decorators are functions that modify the behavior of another function or a class, enhancing their functionality.\n",
    "####\n",
    "#### Here, the chain decorator wraps the runnable some function in a runnable lambda, just as we manually did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d00ab1d-1a3c-481b-b6e0-17ae6451758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def runnable_sum(x):\n",
    "    return sum(x)\n",
    "\n",
    "@chain\n",
    "def runnable_square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e366027-c301-42bc-9466-3f3545d9ba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.runnables.base.RunnableLambda,\n",
       " langchain_core.runnables.base.RunnableLambda)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(runnable_sum), type(runnable_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be71745f-1fe0-4bd8-9d0d-640ccda29631",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = runnable_sum | runnable_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "641bbf9a-f337-4d58-b6c1-868a957dba82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke([1, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40534-935b-4538-ba66-6dbf0e5437fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662c2c14-ff37-4873-84d3-aadc57414fd2",
   "metadata": {},
   "source": [
    "# Adding Memory to a Chain (Part 1): Implementing the Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc30999a-8e44-4f4d-b761-f19e29abcb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0f8ee66-6469-40da-80a0-fdf1ae852010",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{message_log}\n",
    "\n",
    "Human: \n",
    "{question}\n",
    "\n",
    "AI:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44e402a9-9749-4af9-93ce-106ad047f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c65d5c44-5614-44d1-a353-d75a7fc3e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\longchain_envllm\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3519: UserWarning: Parameters {'seed'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name = 'gpt-4', \n",
    "                  model_kwargs = {'seed':365},\n",
    "                  temperature = 0,\n",
    "                  max_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94ff2c1f-a4f0-4b81-84c2-3d14a5889b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37578e85-1bf7-49ab-8311-bf7f84529b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Absolutely! Octopuses are fascinating creatures with a unique cardiovascular system. They have three hearts, each serving a specific purpose. Two of these hearts, known as branchial hearts, are located near each of the octopus's two gills. These branchial hearts pump blood to the gills, where it gets oxygenated.\\n\\nThe third heart, known as the systemic heart, pumps the oxygenated blood from the gills to the rest of the body. This heart actually stops beating when the oct\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message_log':'''The AI provides an interesting fact about octopuses, stating that they have three hearts. \n",
    "Two hearts pump blood to the gills, while the third heart pumps it to the rest of the body.''', \n",
    "              'question':\"Can you elaborate a bit more on this fact?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5bc8bca-090e-43cf-9273-393153193228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13360\\3865083148.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = ConversationSummaryMemory(llm = ChatOpenAI(), memory_key = 'message_log')\n"
     ]
    }
   ],
   "source": [
    "chat_memory = ConversationSummaryMemory(llm = ChatOpenAI(), memory_key = 'message_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8ca76b7-f6ec-4bc3-b4d5-bcccd09356a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': ''}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566085c8-02fe-44ab-ac3e-9f09f564d645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b95f8625-ab87-4e11-b376-0ce3e42a7770",
   "metadata": {},
   "source": [
    "# RunnablePassthrough with Additional Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9ad2a9b-88ff-4a7d-a5b9-0e7a39bb08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6309fd4-41dc-4b6a-8c46-0699eea27926",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{message_log}\n",
    "\n",
    "Human: \n",
    "{question}\n",
    "\n",
    "AI:\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9e09241-8a93-450c-b88b-90a2f68d8edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\longchain_envllm\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3519: UserWarning: Parameters {'seed'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name = 'gpt-4', \n",
    "                  model_kwargs = {'seed':365},\n",
    "                  temperature = 0,\n",
    "                  max_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ad6632f-eeb6-45d2-9bc8-cce4b0508742",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ff3f7bb-4b30-41f4-883f-21f2ad999c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory = ConversationSummaryMemory(llm = ChatOpenAI(), memory_key = 'message_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f9a9e6f-57be-44bb-985f-af49d5507953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': ''}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b16f37cc-449a-4c88-bafd-3708926e4e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you give me an interesting fact I probably didn't know about?\",\n",
       " 'message_log': {'message_log': ''}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(message_log = chat_memory.load_memory_variables).invoke(\n",
    "    {'question':\"Can you give me an interesting fact I probably didn't know about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df7460e0-14e9-4fbc-98b5-88ac3204c208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'hi', 'first_letter': 'h', 'second_letter': 'i'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(first_letter = lambda x: list(x['input'])[0], \n",
    "                           second_letter = lambda x: list(x['input'])[1]\n",
    "                          ).invoke({'input':'hi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c46e2-9c09-48af-a20e-831db6585d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longchain_envllm",
   "language": "python",
   "name": "longchain_envllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
